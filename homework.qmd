---
title: "NY Taxi Data in Spark"
author: "Lee Durbin"
format: html
---

## Introduction

This notebook demonstrates how to work with data in Spark via the sparklyr package. The purpose of this exercise is to complete the Module 5 homework for the Data Engineering Zoomcamp.

## Getting Started

First, we install the sparklyr package and load it in R. Let's also make sure we have the dplyr, readr, fs, and lubridate packages too, which will help us:

```{r prerequisites}
#| eval: false

install.packages(c("sparklyr", "dplyr", "readr", "fs", "lubridate"))

sparklyr::spark_install()
```

Now, let's set an environment variable to point to where we've installed Java. Then we can connect to Spark and we're ready to answer the homework questions:

```{r start_spark}
#| output: false

Sys.setenv(JAVA_HOME = "/usr/local/Cellar/openjdk@17/17.0.9")

library(sparklyr)

sc <- spark_connect(
  master = "local",
  spark_home = "/usr/local/Cellar/apache-spark/3.5.0/libexec"
  )
```


## Question 1

The first question just requires that we return the version of Spark that we're using:

```{r q1}
sparklyr::spark_version(sc) 
```


## Question 2

Let's read the October 2019 FHV data into a Spark Dataframe:

```{r read_dataframe}
tbl_fhv <- spark_read_csv(
  sc,
  name = "spark_fhv2019",
  path = fs::dir_ls(here::here("data"), glob = "*.csv.gz"),
  overwrite = TRUE
)
```

Next, we'll repartition the Dataframe to 6 partitions and save it to parquet:

```{r partition_to_parquet}
fhv_partitioned <- sdf_repartition(tbl_fhv, 6)

parquet_dir <- here::here("data/fhv/2019/10/")

sparklyr::spark_write_parquet(fhv_partitioned, parquet_dir, mode = "overwrite")
```

The answer for Question 2 requires us to get the average file size of the Parquet Files. Here's how we do that:

```{r partition_sizes}
file_info <- file.info( fs::dir_ls(parquet_dir, glob = "*.snappy.parquet"))

mean(file_info$size) 
```


## Question 3

For this question we need to filter the data to return only those trips that started on 15 October.

First, let's use Spark to read the Parquet files we previously saved:

```{r read_parquet}
tbl_fhv_partitioned <- spark_read_parquet(
  sc,
  name = "spark_fhv2019_partitioned",
  path = parquet_dir)
```

Now we apply the filter using some SQL and count the number of rows returned:

```{r filter_15_oct}
library(DBI)

dbGetQuery(
  sc,
  "SELECT * FROM spark_fhv2019_partitioned WHERE date(from_utc_timestamp(to_timestamp(pickup_datetime), 'America/New_York')) = '2019-10-15'"
  ) |> 
  nrow()
```


## Question 4

What is the length of the longest trip in the dataset in hours?

To determine this, we first need to calculate the number of hours for each trip. Unfortunately we can't use lubridate functions when we're querying Spark, but by calling unix_timestamp() on the pickup and dropoff times and then dividing by 3,600 we can calculate the difference ourselves:


```{r hours_difference}
fhv_hours_diff <- tbl_fhv_partitioned |> 
  mutate(
    hours_difference = (
      unix_timestamp(dropOff_datetime) - unix_timestamp(pickup_datetime)
      ) / 3600
    )
```

To get the answer, we'll sort our table by this newly-created column in descending order and return the first value:

```{r q4_answer}
fhv_hours_diff_arranged <- fhv_hours_diff |> 
  dplyr::arrange(desc(hours_difference)) |> 
  head(1) |> 
  collect()

fhv_hours_diff_arranged$hours_difference
```


## Question 6

Now let's load the zone lookup data into a temp view in Spark:

```{r load_zones}
#| output: false

zones <- spark_read_csv(
  sc,
  name = "spark_fhv_zones",
  path = here::here("data/fhv_zones.csv"),
  overwrite = TRUE
)

```

Next, let's join the spark_fhv_zones table with the spark_fhv2019_partitioned table:

```{r join_tables}
fhv_with_zones <- left_join(
  tbl_fhv_partitioned,
  zones,
  by = c("PUlocationID" = "LocationID"))

fhv_with_zones
```

What is the name of the LEAST frequent pickup location Zone? To determine this, we'll group by Zone and count the rows:

```{r}
zone_counts <- fhv_with_zones |> 
  dplyr::group_by(Zone) |> 
  dplyr::summarise(count = n()) |> 
  dplyr::arrange(count) |> 
  head(1) |> 
  collect()
```

From this we can see that the answer is `r zone_counts$Zone`.

## Finish

Let's make sure we close the connection to Spark now that we're done:

```{r}
spark_disconnect(sc)
```

